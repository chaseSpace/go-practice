# 说明

本文总结自：https://time.geekbang.org/column/intro/100839101

[Github免费课程](https://github.com/uaxe/geektime-docs/blob/master/AI-大数据/程序员的AI开发第一课/docs/07-LangChain：一个AI应用开发生态.md)

## K1 你应该知道的LLM基础知识-用户视角

1. 知识丰富、理解上下文、知错就改、守法守德
2. GPT 模型是一个大语言模型，LLM，核心就是大，即训练时输入了海量的文本。

## K2 你应该知道的LLM基础知识-技术视角

1. 大模型的工作很简单，一次添加一个词（Token）。
2. 大模型的本质工作做的是针对任何文本产生“合理的延续”
3. 合理延续就是不断提供下一个合理的词。
4. 关键指标：上下文窗口，即模型可以记住多少个词。能处理的 Token 越多，大模型对信息的理解就越充分，生成的内容就越接近我们需要的结果
5. 关键指标：温度。温度越低，输出越稳定，温度越高，输出越随机。
6. 关键词：Embedding。大模型内部处理的单元是向量（各种算法的输入类型），而用户提问是文本，就需要一种方法将文本转化为向量。那就是
   Embedding。
    - 6.1 Embedding是把文本转化为向量，然后通过向量来计算相似度，从而实现文本的检索。

## K3 怎样与大模型沟通

提示词：定义角色+背景信息+任务目标+输出要求。

## K4 提示工程：更好地释放LLM的能力

为了提升大模型输出的质量，我们需要引入提示工程。上一节说的提示词就是提示工程的一个简单例子。

在商业化场景中，我们需要让大模型处理一些复杂任务，然后输出结构化的回复，这种单靠提示词是无法实现的，需要引入一些更复杂的提示工程。

### 1. 零样本提示

做一些简单的任务，我们不需要给大模型提供任何提示词，大模型会自动生成一些符合要求的结果。例如回答“1+1等于几”，“苹果和柠檬哪个甜”。

### 2. 少样本提示

接下来看一下稍微复杂的任务：“1+1+a等于几（我们假设26个字母中，前一半的字母代表1，后一半的字母代表2）”，
这里我们需要给大模型提供一些括号内的提示词，让大模型知道如何处理这种不那么常规的任务。

### 3. 思维链提示

大模型的优势在于语言处理，劣势在于数学、推理，经常看到大模型在一些简单的数学问题上闹笑话，比如 3.8 和 3.11 到底谁大。

这种情况可以认为是大模型未经思考的输出，此时我们考虑让大模型给出每一步的思考步骤，最后输出结果，就能大大提升大模型的输出结果的准确性。
此时我们可以在问题前面加上提示词：“下面的问题请你给出思考步骤，然后输出结果：”

### 4. ReAct 框架

ReAct 实际上是两个单词的缩写：Reasoning + Acting，也就是推理+行动。该框架将大模型推理和一些行动能力结合起来，超越大模型自身的限制。

## K4 OpenAI API：LLM编程的事实标准（上）

当我们要接入LLM API时，只需参考业界前沿产品OPEN AI API的文档，即可快速接入。其他大模型的接口也都是类似的。

最简单的是文本补全接口：

```
curl https://api.openai.com/v1/chat/completions
-H "Content-Type: application/json"
-H "Authorization: Bearer $OPENAI_API_KEY"
-d '{
        "model": "gpt-4o",
        "messages": [
            {"role": "user", "content": "写一首关于AI的诗"}
        ]
    }'
```

大模型还有其他功能（不同接口）：

- Text Generation：生成和处理文本
- Embeddings：文本转向量
- Speech to Text：语音转文本
- Image Generation：生成图像
- Vision：处理图像输入

注意，即使是接入OPEN AI的接口，以下2个参数也是统一的，一般通过配置方式传入：

```
export OPENAI_API_BASE="your_api_base_here"
export OPENAI_API_KEY="your_api_key_here"
```

### 1. 核心参数

文本补全接口的路径是：`/v1/chat/completions`，传入的通常是一个消息列表。messages可以简单理解为一个历史聊天记录，
每个消息由一个role和content组成，role可以是user、assistant（模型回复）、system（开发者提示词）。

还有其他参数：

- temperature：设定大模型回复的确定性，值越小，表示确定性越强，值越大，表示随机性越强（不同模型的取值范围不同，一般是0~2）。
- max_completion_tokens，它表示生成应答的最大 token 数。大模型生成内容通常是按照 token 数计费
- stream，是否需要流式应答。流式应答主要是为了提升聊天的响应速度

### 2. 工具参数

在请求参数中，有一部分是出于工程目的提供的：

- **user**，终端用户标识，它是我们作为开发者提供给OpenAI 的，主要就是用作监控和检测 API 的滥用，监控粒度就到了个体上。
- **n**，为每条输入消息生成多少个回复。虽然看上去可以生成更多内容，但生成内容要计费，所以，如果没有特别需求，就不要额外设置这个参数。
- **response_format**，应答格式。缺省情况下，这个接口只生成文本内容。但对开发来说，我们经常会用到 JSON
  格式。我们当然可以用提示词要求大模型返回，也可以通过设置 response_format 让 API 直接返回 JSON 格式

### 3. 工具参数

除了常规的聊天，目前大模型还有一种常见用法，就是构建 Agent。Agent 一个很重要的用法就是扩展大模型的边界，让它能做更多的事情，比如，查询天气、搜索网页等等。

在讲 ReAct 框架时，提到了怎样在提示词里写可以调用的工具（最开始它叫函数），而 OpenAI API 给我们提供了另外一种做法，通过工具参数进行传递。

工具参数里最常用的是这两个：

- **tools**，模型可以调用的工具列表。其中的每个工具都会包含 type（类型）和 function（函数）两个部分。目前 type 表示工具的类型，目前只有
  function 一个类型。function 主要用来告诉模型函数可以怎样调用，包含 description（函数的描述），name（函数名）以及parameters（函数参数）。
- **tool_choice**，选择怎样调用工具。参数值为 none 表示不调用工具，auto 表示模型自行选择是生成消息，还是调用工具，required
  表示必须调用工具。这个参数值也可以是一个对象，比如，下面这行代码就告诉模型，要调用我指定的这个工具。

### 4. 模型参数

还有一大堆参数是提供给模型开发者的，不是是模型应用开发者的，比如：seed、stop、frequency_penalty、logit_bias。。

## K5 OpenAI API：LLM编程的事实标准（下）

大模型回复的方式有两种：普通HTTP应答、流式应答。

### 1. 普通应答

正常的 HTTP 应答：

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-4o-mini",
  "system_fingerprint": "fp_44709d6fcb",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "\n\nHello there, how may I assist you today?"
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}
```

我们先来了解几个简单的参数：

- **id**，应答的唯一标识。这个比较好理解，方便在出问题时定位问题。
- **object**，对象类型。这是 OpenAI API 应答的一个通用字段，不同类型的应答都会有自己固定的对象类型，在聊天补全接口中，它的值就是
  chat.completion。
- **created**，Unix 时间戳。它表明了这个应答生成的时间。
- **model**，生成应答的模型。大部分情况下，它就是请求时所带的模型。不过，同一个模型可能存在不同版本的情况，它有时会返回具体的版本，比如：gpt-4o-mini-2024-07-18。
- **system_fingerprint**，系统指纹。它代表了模型运行时使用的后端配置。在讲到请求中的技术参数时，我们提到过一个 seed
  参数，可以当做后端缓存来看。seed 参数就是要与这个 system_fingerprint 配合使用的。

接下来的 choices 才是我们应答的重点，这是大模型给我们回复的真正内容。choices 本身是一个对象列表，其中的每个对象就是大模型生成文本的一部分。我们来具体看一下：

- **index**，索引。这就是一个顺序编号，如果文本被切分了，通过索引就可以将内容重新排列，生成正确的顺序。不过，如果对于标准的
  HTTP
  应答，切片的必要性不大，往往只有一块。
- **finish_reason**，停止生成 token 的原因。文本不会无限生成，总会停下来。到了停止点或遇到停止序列，原因就是
  stop，到了一定的长度，原因就是
  length，生成了工具调用就是 tool_calls。
- **message**，回复的消息。在这个例子中，包含了两个字段：角色（role）和内容（content）。这个部分与请求中的消息是一样的，最核心的字段就是内容，角色部分已经解释过了（可以回看上一讲的核心参数部分）。

### 2. 流式应答

流式应答的出现主要是为了解决大模型生成文本比较慢的问题。如果等大模型把所有内容生成一次性返回，等待的时间会非常长。对于聊天的场景，这会让本已很长的等待时间会显得更加漫长。

接下来的问题就是，流式应答可以采用什么技术实现呢？如果你做过服务端的开发，像这种涉及到服务端主动向客户端推送内容，我们可能会选择
Websocket，但 OpenAI 在这个问题上却选择了 **SSE** 这种技术。

```
SSE 是服务器发送事件（Server-Sent Event），它是一种服务器推送技术，客户端通过 HTTP 连接接收来自服务器的自动更新，它描述了服务器如何在建立初始客户端连接后向客户端发起数据传输。
——Wikipedia
```

SSE 则是可以理解为建立在 HTTP 通信协议基础上的一层简单应用协议。OpenAI 之所以选择 SSE，而非 WebSocket，是因为 SSE
的技术特点刚好可以契合流式应答的需求：客户端与大模型的交互是一次性的，每产生一个
token，服务端就可以给客户端推送一次，**当生成内容结束时，断掉连接，无需考虑客户端的存活情况**。如果采用 WebSocket
的话，服务端就需要维护连接，像 OpenAI 这样的服务体量，维护连接就会造成很大的服务器压力，而且，在生成内容场景下，也没有向服务端进一步发送内容，WebSocket
的双向通信在这里也是多余的。

单就 SSE 这项技术而言，它存在已经很长时间了，2004 年就有人提出，但直到大模型的兴起，这项技术才彻底流行起来。

这里有一点细节的问题，SSE 通常是用在 GET 请求上的，而 OpenAI 的聊天接口是一个 POST 请求，从规范的角度看，它的用法并不完全恰当，只是
OpenAI API 的流行让大家接受了它。如果严格遵守 SSE 程序库处理 OpenAI API ，就可能会遇到无法 POST 请求 SSE 的情况。

SSE 貌似是一项高深的技术，但只要我们看一下报文就不难理解它是如何实现的。根据报文的形式，SSE
通常分成纯数据消息和事件消息。纯数据消息，顾名思义就是只有数据的消息，下面是一个例子：

```
data: This is the first message.

data: This is the second message, it
data: has two lines.

data: This is the third message.
```

每一条消息开头都有一个 data，表示后面的内容就是一条数据。这种形式的数据消息就是流式应答里最常用的消息，每次生成了一个
token，就推送一条以 data 开头的数据块，所以，我们会看到一个又一个的消息块。

事件消息，我们看一个例子也就很容易理解了：

```
event: add
data: 73857293

event: remove
data: 2153

event: add
data: 113411
```

这里的消息会先有一个事件（event），后面跟着具体的数据（data），对于程序员来说，这种做法类似于函数和它的参数。服务端每次推送，都会推送一个事件加上一个数据。